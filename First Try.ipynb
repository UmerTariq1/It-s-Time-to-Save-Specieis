{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,r2_score,accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data\\\\train.csv\")\n",
    "df_test = pd.read_csv(\"data\\\\test.csv\")\n",
    "df_mck = pd.read_excel(\"data\\\\MCK.xlsx\")\n",
    "df_columns = pd.read_excel(\"data\\\\column_comparison.xlsx\")\n",
    "df_country = pd.read_excel(\"data\\\\country_mapping.xlsx\")\n",
    "\n",
    "df = pd.concat([df_train, df_test], keys=['train', 'test'])\n",
    "# df = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_columns = df_columns.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "df = df.drop([\"Intergalactic Development Index (IDI), male, Rank\"],axis=1)\n",
    "df = df.drop([\"Intergalactic Development Index (IDI), female, Rank\"],axis=1)\n",
    "df = df.drop([\"Intergalactic Development Index (IDI), Rank\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape , df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@params:\n",
    "    df=dataframe\n",
    "    targetVariable = Target variable\n",
    "    numberOfColumns = number of values against targetVariable should be compared with (top k variables with highest correlation)\n",
    "    largestCorrelation : true if values with largest corrleation is wanted and false otherwise\n",
    "\"\"\"\n",
    "def getTopCorrelationColumnsNames(df, targetVariable, numberOfColumns, largestCorrelation):\n",
    "    corrmat = df.corr()\n",
    "    if largestCorrelation==True:\n",
    "        return corrmat.nlargest(numberOfColumns, targetVariable)[targetVariable].index\n",
    "    else:\n",
    "        return corrmat.nsmallest(numberOfColumns, targetVariable)[targetVariable].index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topColumns = getTopCorrelationColumnsNames(df_train,\"y\",31,True).tolist()\n",
    "len(topColumns)\n",
    "df_train_short = df_train[topColumns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_short.shape,df_train_short.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape , df_mck.shape , df_columns.shape , df_train_short.shape, df_country.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. combine train test\n",
    "##### 2. rename trainTest\n",
    "##### 3. find std dev of each feature for each country / galaxy on trainTest and mck data\n",
    "##### 4. apply knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainColumns = df_columns[\"TRAIN\"].tolist()\n",
    "trainMckColumns = df_columns[\"MCK\"].tolist()\n",
    "\n",
    "columnDictionary = {}\n",
    "for train_col, trainMck_col in zip(trainColumns,trainMckColumns) :\n",
    "    columnDictionary[train_col] = trainMck_col\n",
    "#     {old_name : new name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxyColumns = df_country[\"galaxy\"].tolist()\n",
    "countryColumns = df_country[\"country\"].tolist()\n",
    "\n",
    "countryDictionary = {}\n",
    "for galaxy_col, country_col in zip(galaxyColumns,countryColumns) :\n",
    "    countryDictionary[galaxy_col] = country_col\n",
    "#     {old_name : new name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df.rename(columns=columnDictionary,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm taking mean\n",
    "originalDfColumns = df_renamed.columns.tolist()\n",
    "\n",
    "df_CombinedGrouped = df_renamed.groupby(\"Country\").mean()\n",
    "\n",
    "df_mck = df_mck[ originalDfColumns[:-1]]\n",
    "                       \n",
    "df_mckGrouped = df_mck.groupby(\"Country\").mean()\n",
    "zColumns = [ (col_name+\"_zscore\") for col_name in df_CombinedGrouped.columns.tolist() ]\n",
    "\n",
    "df_renamed_mean = df_renamed.mean()   #mean of  whole dataset for each feature\n",
    "df_renamed_std = df_renamed.std()    #std of  whole dataset for each feature\n",
    "\n",
    "df_mck_mean = df_mck.mean()   #mean of  whole dataset for each feature\n",
    "df_mck_std = df_mck.std()    #std of  whole dataset for each feature\n",
    "\n",
    "df_CombinedGrouped.shape, df_mckGrouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add columns \n",
    "def addColumn(df,columnName,columnValue=0):\n",
    "    df[columnName] = columnValue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding zscore column to the dataset and each will have a value of 0\n",
    "for cols in zColumns:\n",
    "    df_CombinedGrouped = addColumn(df_CombinedGrouped,cols,0.0)\n",
    "    if \"y_zscore\" not in cols:\n",
    "        df_mckGrouped = addColumn(df_mckGrouped,cols,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_CombinedGrouped.iterrows():\n",
    "    #   col_name is name of the column and its value is that column's mean\n",
    "    for col_name in  df_renamed_mean.index.tolist() :\n",
    "        if not math.isnan(row[col_name]):\n",
    "            df_CombinedGrouped.at[index,(col_name+\"_zscore\")] = ( row[col_name] - df_renamed_mean[col_name] )  / df_renamed_std[col_name]\n",
    "\n",
    "for index, row in df_mckGrouped.iterrows():\n",
    "    #   col_name is name of the column and its value is that column's mean\n",
    "    for col_name in  df_mck_mean.index.tolist() :\n",
    "        if not math.isnan(row[col_name]):\n",
    "            df_mckGrouped.at[index,(col_name+\"_zscore\")] = ( row[col_name] - df_mck_mean[col_name] )  / df_mck_std[col_name]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CombinedGrouped = df_CombinedGrouped.iloc[:,-76:]\n",
    "df_mckGrouped = df_mckGrouped.iloc[:,-75:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountryNames_unqiue = list(set(df_mckGrouped.index.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mckGrouped.fillna(df_mckGrouped.mean(),inplace=True)\n",
    "df_CombinedGrouped.fillna(df_CombinedGrouped.mean(),inplace=True)\n",
    "\n",
    "df_mckGrouped = addColumn(df_mckGrouped,\"Country_encoded\",-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(CountryNames_unqiue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_mckGrouped.iterrows():\n",
    "    df_mckGrouped.at[index,\"Country_encoded\"] = le.transform([index]).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_mckGrouped.drop(['Country_encoded',\"Year_zscore\"],axis=1)\n",
    "y = df_mckGrouped['Country_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=1,algorithm=\"ball_tree\")\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test = df_CombinedGrouped.drop([\"y_zscore\",\"Year_zscore\"],axis=1)\n",
    "# X_test = df_CombinedGrouped.drop([\"y_zscore\",\"Year_zscore\",\"Predicted Country code\"],axis=1)\n",
    "\n",
    "# y_pred = classifier.predict(X_test)\n",
    "df_CombinedGrouped[\"Predicted Country code\"] = classifier.predict(X_test)\n",
    "\n",
    "# y_pred = classifier.kneighbors(X_test,n_neighbors=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FOR kneighbors and K=1\n",
    "# outpred1 = []\n",
    "# outpred2 = []\n",
    "\n",
    "\n",
    "# for i in y_pred[0]:\n",
    "#     outpred1.append(i[0])\n",
    "\n",
    "# for i in y_pred[1]:\n",
    "#     outpred2.append(i[0])\n",
    "\n",
    "\n",
    "\n",
    "# ydict = {\"outpred1\":outpred1}\n",
    "\n",
    "# y_pred = pd.DataFrame(ydict,outpred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FOR kneighbors and K=2\n",
    "\n",
    "# outpred1 = []\n",
    "# outpred2 = []\n",
    "# outpred3 = []\n",
    "# outpred4 = []\n",
    "\n",
    "# for i in y_pred[0]:\n",
    "#     outpred1.append(i[0])\n",
    "\n",
    "# for i in y_pred[0]:\n",
    "#     outpred2.append(i[1])\n",
    "\n",
    "# for i in y_pred[1]:\n",
    "#     outpred3.append(i[0])\n",
    "\n",
    "# for i in y_pred[1]:\n",
    "#     outpred4.append(i[1])\n",
    "\n",
    "# ydict = {\"outpred1\":outpred1,\"outpred2\":outpred2,\"outpred4\":outpred4}\n",
    "\n",
    "# y_pred = pd.DataFrame(ydict,outpred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = pd.DataFrame(y_pred)\n",
    "# y_pred[y_pred.duplicated()]\n",
    "# y_pred[y_pred.index.duplicated()]\n",
    "# y_pred.sort_values(by=\"outpred1\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CombinedGrouped = addColumn(df_CombinedGrouped,\"galaxyToCountry_junaid\",\"\")\n",
    "df_CombinedGrouped = addColumn(df_CombinedGrouped,\"Predicted Country\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df_CombinedGrouped.iterrows():\n",
    "    if index in countryDictionary:\n",
    "        df_CombinedGrouped.at[index,\"galaxyToCountry_junaid\"] = countryDictionary[index]\n",
    "    pred_country_code = row[\"Predicted Country code\"]\n",
    "#     if not pred_country_code== -1:\n",
    "    df_CombinedGrouped.at[index,\"Predicted Country\"] = le.inverse_transform([pred_country_code]).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# finding difference between model predictions and junaid mapping\n",
    "# wrongPredicted = 0\n",
    "# for index,row in df_CombinedGrouped.iterrows():\n",
    "#     if row[\"galaxyToCountry_junaid\"] != row[\"Predicted Country\"]:\n",
    "#         wrongPredicted += 1\n",
    "# wrongPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "galaxyCountryModelMapping = {}\n",
    "for index,row in df_CombinedGrouped.iterrows():\n",
    "    galaxyCountryModelMapping[index] = row[\"Predicted Country\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mck_copy = df_mck.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withModelCountry = df_renamed.copy()\n",
    "# renaming country column to galaxy so it will be easier to merge mck and trainTest datasets\n",
    "df_withModelCountry.rename(columns={\"Country\":\"galaxy\"},inplace=True)\n",
    "df_withModelCountry = addColumn(df_withModelCountry,\"Country\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index,row in df_withModelCountry.iterrows():\n",
    "    countryName = row[\"galaxy\"]\n",
    "    df_withModelCountry.at[index,\"Country\"] = galaxyCountryModelMapping[countryName]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withModelCountry['Year'] = (df_withModelCountry['Year'].rank(method = 'dense') + 1989)\n",
    "df_withModelCountry['Year'] = df_withModelCountry['Year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withModelCountry = df_withModelCountry[df_withModelCountry['Year']>2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_withModelCountry = df_withModelCountry.astype(str)\n",
    "# df_mck_copy = df_mck_copy.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfTrainTestMck_merged = pd.merge(df_withModelCountry, df_mck_copy,  how='left', left_on=['Country','Year'], right_on = ['Country','Year'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Have the Merged dataset. ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#taking year to the end for future processing\n",
    "dfTrainTestMck_merged = dfTrainTestMck_merged[[c for c in dfTrainTestMck_merged if c not in ['Year']] \n",
    "       + ['Year']]\n",
    "\n",
    "#merged, arranged (year country at start and y at end) , will be used for regression analysis\n",
    "mck_merged = dfTrainTestMck_merged.iloc[:,75:]\n",
    "mck_merged = mck_merged[ ['Year'] + [c for c in mck_merged if c not in ['Year']]  ]  # just taking year column at start of df\n",
    "mck_merged = addColumn(mck_merged,\"Country_encoded\",\"\")\n",
    "\n",
    "for index, row in mck_merged.iterrows():\n",
    "    countryName = row[\"Country\"]\n",
    "    mck_merged.loc[index,\"Country_encoded\"] = le.transform([countryName]).tolist()[0]\n",
    "\n",
    "mck_merged = mck_merged[ [c for c in mck_merged if c not in ['y']] + ['y'] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrainTestMck_merged = dfTrainTestMck_merged[['Year'] + [c for c in dfTrainTestMck_merged if c not in ['Year']] ]\n",
    "dfTrainTest_merged = dfTrainTestMck_merged.iloc[:,:78]\n",
    "\n",
    "#below 2 lines are just arranging column positions\n",
    "dfTrainTest_merged = dfTrainTest_merged[ ['Year','Country'] + [c for c in dfTrainTest_merged if c not in ['Year','Country']]  ]  # just taking year column at start of df\n",
    "\n",
    "dfTrainTest_merged = addColumn(dfTrainTest_merged,\"Country_encoded\",\"\")\n",
    "for index, row in dfTrainTest_merged.iterrows():\n",
    "    countryName = row[\"Country\"]\n",
    "    dfTrainTest_merged.at[index,\"Country_encoded\"] = le.transform([countryName]).tolist()[0]\n",
    "\n",
    "dfTrainTest_merged = dfTrainTest_merged[ [c for c in dfTrainTest_merged if c not in ['y']] + ['y']   ]  # just taking year column at start of df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mck_merged.drop(['y','Country'], axis=1)\n",
    "y_train = mck_merged['y']\n",
    "X_train.fillna(X_train.mean(),inplace=True)\n",
    "\n",
    "X_test = dfTrainTest_merged.drop(['y','Country','galaxy'], axis=1)\n",
    "y_test = dfTrainTest_merged['y']\n",
    "X_test.fillna(X_test.mean(),inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred_reg = reg.predict(X_test)\n",
    "\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_reg)))\n",
    "r2 = r2_score(y_test, y_pred_reg)\n",
    "adjR2 = 1-(1-r2)*((X_train.shape[0])-1)/((X_train.shape[0])-(X_train.shape[1])-1)\n",
    "\n",
    "print(\"RMSE: %f\" % rmse)\n",
    "print(\"R2 score = \",r2)        # r2 score \n",
    "print(\"Adjusted R2 score = \",adjR2)\n",
    "\n",
    "correctCounter = 0\n",
    "for i in range(len(y_test)):\n",
    "    if round(y_test[i],1) == round(y_pred_reg[i],0):\n",
    "        correctCounter+=1\n",
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCR\n",
    "clf = svm.SVR( gamma='scale', C=1.0, epsilon=0.2)\n",
    "clf.fit(X_train,y_train) \n",
    "y_pred_reg = clf.predict(X_test)\n",
    "\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_reg)))\n",
    "r2 = r2_score(y_test, y_pred_reg)\n",
    "adjR2 = 1-(1-r2)*((X_train.shape[0])-1)/((X_train.shape[0])-(X_train.shape[1])-1)\n",
    "\n",
    "print(\"RMSE: %f\" % rmse)\n",
    "print(\"R2 score = \",r2)        # r2 score \n",
    "print(\"Adjusted R2 score = \",adjR2)\n",
    "\n",
    "correctCounter = 0\n",
    "for i in range(len(y_test)):\n",
    "    if round(y_test[i],1) == round(y_pred_reg[i],0):\n",
    "        correctCounter+=1\n",
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGboost\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 1, learning_rate = 0.5,\n",
    "                max_depth = 1000, alpha = 10, n_estimators = 10)\n",
    "xg_reg.fit(X_train.values,y_train)\n",
    "y_pred_reg = xg_reg.predict(X_test.values)\n",
    "\n",
    "\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_reg)))\n",
    "r2 = r2_score(y_test, y_pred_reg)\n",
    "adjR2 = 1-(1-r2)*((X_train.shape[0])-1)/((X_train.shape[0])-(X_train.shape[1])-1)\n",
    "\n",
    "print(\"RMSE: %f\" % rmse)\n",
    "print(\"R2 score = \",r2)        # r2 score \n",
    "print(\"Adjusted R2 score = \",adjR2)\n",
    "\n",
    "correctCounter = 0\n",
    "for i in range(len(y_test)):\n",
    "    if round(y_test[i],1) == round(y_pred_reg[i],0):\n",
    "        correctCounter+=1\n",
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 9, 'objective': 'reg:linear', 'base_score':0.2, 'booster':'gbtree', 'colsample_bylevel':1,\n",
    "         'colsample_bytree':0.8}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'rmse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train.values, label=y_train, missing=-999.0)\n",
    "dtest = xgb.DMatrix(X_test.values, label=y_test, missing=-999.0)\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "num_round = 50\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for missing = -999 :                                             eval-rmse:0.119211\ttrain-rmse:0.004722\n",
    "# for missing 0 :                                                 eval-rmse:0.116161\ttrain-rmse:0.004726\n",
    "# for missing 0 and base_score:0.2:                               eval-rmse:0.084709\ttrain-rmse:0.004723\n",
    "# for missing -999 and base_score:0.2:                            eval-rmse:0.087144\ttrain-rmse:0.00472\n",
    "# for missing -999 and base_score:0.2:                            eval-rmse:0.087144\ttrain-rmse:0.00472\n",
    "# for missing -999 and base_score:0.2 and 'max_depth': 10:        eval-rmse:0.083773\ttrain-rmse:0.004708\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest neighbours\n",
    "knn = neighbors.KNeighborsRegressor(5)\n",
    "knn.fit(X_train.values,y_train)\n",
    "y_pred_reg = knn.predict(X_test.values)\n",
    "\n",
    "\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_reg)))\n",
    "r2 = r2_score(y_test, y_pred_reg)\n",
    "adjR2 = 1-(1-r2)*((X_train.shape[0])-1)/((X_train.shape[0])-(X_train.shape[1])-1)\n",
    "\n",
    "print(\"RMSE: %f\" % rmse)\n",
    "print(\"R2 score = \",r2)        # r2 score \n",
    "print(\"Adjusted R2 score = \",adjR2)\n",
    "\n",
    "correctCounter = 0\n",
    "for i in range(len(y_test)):\n",
    "    if round(y_test[i],1) == round(y_pred_reg[i],2):\n",
    "        correctCounter+=1\n",
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble\n",
    "reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\n",
    "reg2 = RandomForestRegressor(random_state=1, n_estimators=10)\n",
    "reg3 = LinearRegression()\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_reg = ereg.predict(X_test.values)\n",
    "\n",
    "\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_reg)))\n",
    "r2 = r2_score(y_test, y_pred_reg)\n",
    "adjR2 = 1-(1-r2)*((X_train.shape[0])-1)/((X_train.shape[0])-(X_train.shape[1])-1)\n",
    "\n",
    "print(\"RMSE: %f\" % rmse)\n",
    "print(\"R2 score = \",r2)        # r2 score \n",
    "print(\"Adjusted R2 score = \",adjR2)\n",
    "\n",
    "correctCounter = 0\n",
    "for i in range(len(y_test)):\n",
    "    if round(y_test[i],1) == round(y_pred_reg[i],2):\n",
    "        correctCounter+=1\n",
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\n",
    "gbr.fit(X_train.values,y_train)\n",
    "y_pred_reg = gbr.predict(X_test.values)\n",
    "\n",
    "\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_reg)))\n",
    "r2 = r2_score(y_test, y_pred_reg)\n",
    "adjR2 = 1-(1-r2)*((X_train.shape[0])-1)/((X_train.shape[0])-(X_train.shape[1])-1)\n",
    "\n",
    "print(\"RMSE: %f\" % rmse)\n",
    "print(\"R2 score = \",r2)        # r2 score \n",
    "print(\"Adjusted R2 score = \",adjR2)\n",
    "\n",
    "correctCounter = 0\n",
    "for i in range(len(y_test)):\n",
    "    if round(y_test[i],1) == round(y_pred_reg[i],2):\n",
    "        correctCounter+=1\n",
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n",
    "gbr.fit(X_train, y_train) # fit additional 100 trees to est\n",
    "y_pred_reg = gbr.predict(X_test.values)\n",
    "\n",
    "\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_reg)))\n",
    "r2 = r2_score(y_test, y_pred_reg)\n",
    "adjR2 = 1-(1-r2)*((X_train.shape[0])-1)/((X_train.shape[0])-(X_train.shape[1])-1)\n",
    "\n",
    "print(\"RMSE: %f\" % rmse)\n",
    "print(\"R2 score = \",r2)        # r2 score \n",
    "print(\"Adjusted R2 score = \",adjR2)\n",
    "\n",
    "correctCounter = 0\n",
    "for i in range(len(y_test)):\n",
    "    if round(y_test[i],1) == round(y_pred_reg[i],2):\n",
    "        correctCounter+=1\n",
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = addColumn(df_test,\"y\",\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_renamed = df_test.rename(columns=columnDictionary,inplace=False)\n",
    "df_test_renamed = addColumn(df_test_renamed,\"Country Mapped\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_renamed = addColumn(df_test_renamed,\"Country_encoded\",-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index,row in df_test_renamed.iterrows():\n",
    "    countryName = row[\"Country\"]\n",
    "    df_test_renamed.at[index,\"Country Mapped\"] = galaxyCountryModelMapping[countryName]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_test_renamed.iterrows():\n",
    "    countryName = row[\"Country Mapped\"]\n",
    "    df_test_renamed.at[index,\"Country_encoded\"] = le.transform([countryName]).tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_renamed.fillna(df_test_renamed.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_renamed['YearRanked'] = (df_test_renamed['Year'].rank(method = 'dense') + 1989)\n",
    "df_test_renamed['Year'] = df_test_renamed['Year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_columns = df_columns.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "df_test_renamed = df_test_renamed.drop([\"Intergalactic Development Index (IDI), male, Rank\"],axis=1)\n",
    "df_test_renamed = df_test_renamed.drop([\"Intergalactic Development Index (IDI), female, Rank\"],axis=1)\n",
    "df_test_renamed = df_test_renamed.drop([\"Intergalactic Development Index (IDI), Rank\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_test = df_test_renamed.drop(['Country','Country Mapped',\"Year\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example :\n",
    "# fileName = \"pred.txt\"\n",
    "# mylist = [1,2,3,4]\n",
    "\n",
    "def writeTofile(fileName,mylist):\n",
    "    with open(fileName, 'w') as f:\n",
    "        for item in mylist:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_reg = gbr.predict(X_test_test.values)\n",
    "pd.DataFrame(y_pred_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_reg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "890"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saadpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = pd.DataFrame(saadpred)\n",
    "gs.to_excel(\"saadpred.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
